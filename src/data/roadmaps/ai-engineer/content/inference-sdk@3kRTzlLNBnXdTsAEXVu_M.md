# Inference SDK

Inference is the process of using a trained model to make predictions on new data. As this process can be compute-intensive, running on a dedicated server can be an interesting option. The huggingface_hub library provides an easy way to call a service that runs inference for hosted models. There are several services you can connect to:

Visit the following resources to learn more:

- [@official@Hugging Face Inference Client](https://huggingface.co/docs/huggingface_hub/en/package_reference/inference_client)
- [@official@Hugging Face Inference API](https://huggingface.co/docs/api-inference/en/index)
